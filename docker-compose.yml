services:
  llm-gateway:
    build:
      context: .
      dockerfile: Dockerfile
    image: llm-gateway:latest
    container_name: llm-gateway
    restart: unless-stopped
    ports:
      - "9000:9000"
    volumes:
      # Mount providers.json - edit this file to configure your providers
      - ./providers.json:/app/providers.json:ro
      # Mount models_fallback_rules.json for fallback rules configuration
      - ./models_fallback_rules.json:/app/models_fallback_rules.json:ro
      # Mount database directory for persistence
      - ./data/db:/app/db
    environment:
      # Required
      - GATEWAY_API_KEY=your-secure-api-key (you can make this up)

      # Optional gateway settings
      - GATEWAY_PORT=9000
      - GATEWAY_HOST=0.0.0.0
      - LOG_FILE_LIMIT=15
      - LOG_CHAT_ENABLED=false
      - FALLBACK_PROVIDER=openrouter

      # Provider API keys - uncomment and set the ones you need
      - APIKEY_OPENROUTER=your-openrouter-api-key (given to you by OpenRouter)
      # - APIKEY_OPENAI=your-openai-api-key
      # - APIKEY_GOOGLE=your-google-api-key
      # - APIKEY_NEBIUS=your-nebius-api-key
      # - APIKEY_TOGETHER=your-together-api-key
      # - APIKEY_KLUSTERAI=your-klusterai-api-key
      # - APIKEY_REQUESTY=your-requesty-api-key
      # - APIKEY_XAI=your-xai-api-key
    healthcheck:
      test: ["CMD", "python", "/app/healthcheck.py"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - llm-gateway-network

networks:
  llm-gateway-network:
    driver: bridge